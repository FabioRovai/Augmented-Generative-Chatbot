{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Augmented Generative Chatbot.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nyplOnH0p-G4",
        "outputId": "9834beae-d9fe-4fdc-a7e5-9015ad00bfac"
      },
      "source": [
        "# Before running this project, you might want to turn on the colab gpu (Runtime>Change runtime type and select GPU as Hardware accelerator).\n",
        "# Choose tensorflow 1.x\n",
        "%tensorflow_version 1.x"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pq63NCnjGzpP"
      },
      "source": [
        "Augmented Generative Chatbot\n",
        "=============="
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uEAIjJX22szV",
        "outputId": "05f8c04b-c814-4fb3-fcad-9b28961499f9"
      },
      "source": [
        "# Install Newspaper3k, NLTK, GPT-2, NLPAug and Glove model\n",
        "!pip install newspaper3k --quiet\n",
        "!pip install nltk --quiet\n",
        "!pip install gpt-2-simple --quiet\n",
        "!pip install NLPAug --quiet\n",
        "!wget https://github.com/allenai/spv2/raw/master/model/glove.6B.100d.txt.gz --quiet\n",
        "!gzip -d /content/glove.6B.100d.txt.gz "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K     |█▌                              | 10kB 28.2MB/s eta 0:00:01\r\u001b[K     |███                             | 20kB 33.3MB/s eta 0:00:01\r\u001b[K     |████▋                           | 30kB 38.2MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 40kB 36.2MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 51kB 38.1MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 61kB 37.7MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 71kB 29.3MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 81kB 27.2MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 92kB 28.8MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 102kB 25.6MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 112kB 25.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 122kB 25.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 133kB 25.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 143kB 25.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 153kB 25.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 163kB 25.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 174kB 25.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 184kB 25.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 194kB 25.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 204kB 25.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 215kB 25.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 92kB 12.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 81kB 8.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 7.4MB 25.5MB/s \n",
            "\u001b[?25h  Building wheel for feedfinder2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for tinysegmenter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for jieba3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for gpt-2-simple (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 389kB 17.4MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QiF3egP0zmjG",
        "outputId": "187553e5-d1af-4eac-f890-7bb2a7c7fd9e"
      },
      "source": [
        "import numpy as np \n",
        "import nltk \n",
        "import random\n",
        "from random import randint\n",
        "import re\n",
        "import nlpaug.augmenter.char as nac\n",
        "import nlpaug.augmenter.word as naw\n",
        "import nlpaug.augmenter.sentence as nas\n",
        "import nlpaug.flow as naf\n",
        "from nlpaug.util import Action\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk.data\n",
        "import gpt_2_simple as gpt2\n",
        "import newspaper\n",
        "from newspaper import Article\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9MKENLEjokjO",
        "outputId": "f6869f80-1fab-40d2-ce75-f3db2ddc272a"
      },
      "source": [
        "# Download different NLTK modules \n",
        "nltk.download ('punkt', quiet=True) \n",
        "nltk.download ('stopwords', quiet=True)\n",
        "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
        "nltk.download('wordnet', quiet=True)\n",
        "\n",
        "# GPT2 Model options: \n",
        "gpt2.download_gpt2(model_name = \"355M\") \n",
        "#gpt2.download_gpt2(model_name = \"124M\") \n",
        "#gpt2.download_gpt2(model_name = \"744M\") "
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fetching checkpoint: 1.05Mit [00:00, 206Mit/s]                                                      \n",
            "Fetching encoder.json: 1.05Mit [00:00, 76.1Mit/s]                                                   \n",
            "Fetching hparams.json: 1.05Mit [00:00, 382Mit/s]                                                    \n",
            "Fetching model.ckpt.data-00000-of-00001: 1.42Git [00:05, 270Mit/s]                                  \n",
            "Fetching model.ckpt.index: 1.05Mit [00:00, 284Mit/s]                                                \n",
            "Fetching model.ckpt.meta: 1.05Mit [00:00, 165Mit/s]                                                 \n",
            "Fetching vocab.bpe: 1.05Mit [00:00, 200Mit/s]                                                       \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7joVs6pqD09"
      },
      "source": [
        "\n",
        "# Adopted from https://www.geeksforgeeks.org/scraping-websites-with-newspaper3k-in-python/\n",
        "\n",
        "# Defining a list of urls \n",
        "# If error is raised for the first article, please run this code again. \n",
        "#The error should non persist after 3-4 attempts. \n",
        "#If this procedure does not fix the error, download/load the file runing the next cell.\n",
        "\n",
        "list_of_urls = ['https://www.tctmagazine.com/additive-manufacturing-3d-printing-news/interview-kadine-james-hobs-3d-printing-studio/', \n",
        "                'https://www.womenofwearables.com/new-blog/wow-woman-in-tech-and-3d-vr-and-ar-kadine-james-learner-maker-innovator-3d-technology-enthusiast',\n",
        "                'https://www.fabbaloo.com/blog/2018/5/9/kadine-james-the-fourth-industrial-revolution-is-changing-the-way-we-receive-information-how-we-process-it-how-we-work-and-what-jobs-we-will-do-it-is-happening-at-an-unprecedented-pace'] \n",
        "  \n",
        "  \n",
        "  # Parse through each url and display its content \n",
        "\n",
        "for url in list_of_urls: \n",
        "    url_i = newspaper.Article(url=\"%s\" % (url), language='en') \n",
        "    url_i.download() \n",
        "    url_i.parse() \n",
        "    url_i.nlp() \n",
        "    count = 0\n",
        "\n",
        "    # Adopted from https://stackoverflow.com/questions/36571560/directing-print-output-to-a-txt-file\n",
        "    \n",
        "    print(url_i.text, file=open(\"output.txt\", \"a\"))\n",
        "\n",
        "    # Backup copy \n",
        "    print(url_i.text, file=open(\"output_backup.txt\", \"a\"))\n",
        "\n",
        "\n",
        "     \n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 344
        },
        "id": "O3ipFdDAx1jW",
        "outputId": "81385804-f50a-405f-fe5e-bbb0d1ac32ed"
      },
      "source": [
        "# Download output_backup.txt if previous cell does not work.\n",
        "# Adapted from https://medium.com/@acpanjan/download-google-drive-files-using-wget-3c2c025a8b99\n",
        "'''!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1F4fjhDPToGjvxTFeoEl441y8oeEk9O5S' -O output.txt\n",
        "'''\n",
        "\n",
        "# or load it from your device\n",
        "'''from google.colab import files\n",
        "files.upload()'''\n",
        "# or, if this github has been cloned, the file should be here Examples_outputs/Output_backup/\n",
        "# Make sure the file is placed into the /content/ colab folder, otherwise move it or change all paths.\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-01-10 15:51:53--  https://docs.google.com/uc?export=download&id=1F4fjhDPToGjvxTFeoEl441y8oeEk9O5S\n",
            "Resolving docs.google.com (docs.google.com)... 172.217.15.78, 2607:f8b0:4004:810::200e\n",
            "Connecting to docs.google.com (docs.google.com)|172.217.15.78|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-0k-0o-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/o4dnsljvh5n3gt18a2b02srqnoqcfskf/1610293875000/02428310132117897738/*/1F4fjhDPToGjvxTFeoEl441y8oeEk9O5S?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2021-01-10 15:51:53--  https://doc-0k-0o-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/o4dnsljvh5n3gt18a2b02srqnoqcfskf/1610293875000/02428310132117897738/*/1F4fjhDPToGjvxTFeoEl441y8oeEk9O5S?e=download\n",
            "Resolving doc-0k-0o-docs.googleusercontent.com (doc-0k-0o-docs.googleusercontent.com)... 172.217.164.129, 2607:f8b0:4004:814::2001\n",
            "Connecting to doc-0k-0o-docs.googleusercontent.com (doc-0k-0o-docs.googleusercontent.com)|172.217.164.129|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 23803 (23K) [text/plain]\n",
            "Saving to: ‘output_backup.txt’\n",
            "\n",
            "output_backup.txt   100%[===================>]  23.25K  --.-KB/s    in 0.006s  \n",
            "\n",
            "2021-01-10 15:51:53 (4.06 MB/s) - ‘output_backup.txt’ saved [23803/23803]\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'from google.colab import files\\nfiles.upload()'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jC5FAHAdLtA"
      },
      "source": [
        "# Cleaning content, adopted from week 5.2\n",
        "#load output.txt or output_backup.txt\n",
        "\n",
        "df = open('/content/output.txt').read()\n",
        "stopWords = set (stopwords.words('english'))\n",
        "\n",
        "def clean_sentence(val):\n",
        "  regex = re.compile('([^\\s\\w\\.]|_)+')\n",
        "  sentence = regex.sub('', val)#.lower()\n",
        "  sentence = sentence.split(\" \")\n",
        "\n",
        "  for word in list (sentence):\n",
        "    if word in stopWords:\n",
        "      sentence.remove(word)\n",
        "\n",
        "      sentence=\" \".join(sentence)\n",
        "\n",
        "      return sentence\n",
        "\n",
        "df = clean_sentence(df)\n",
        "\n",
        "print(df, file=open(\"output_clean.txt\", \"a\"))\n",
        "\n",
        "#?df\n",
        "\n",
        "# This will tokenize our corpus, transforming it into a list\n",
        "\n",
        "df = nltk.sent_tokenize(df) \n",
        "\n",
        "#create backup for later\n",
        "df_backup = df.copy()\n",
        "\n",
        "print(df, file=open(\"output_token.txt\", \"a\")) "
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLarCQ_11yOl"
      },
      "source": [
        "\n",
        "# Shuffling words \n",
        "count = 0\n",
        "while count < 5:\n",
        "    \n",
        "    random.shuffle(df)\n",
        "    print(df, file=open(\"output_shuffled.txt\", \"a\")) \n",
        "    count = count + 1 \n",
        "\n",
        "df = open('/content/output_shuffled.txt').read()    \n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgBoK1AhOhrS"
      },
      "source": [
        "# Leveraging non-contextual embeddings (GloVe) to apply augmentation\n",
        "\n",
        "# Adapted from https://neptune.ai/blog/data-augmentation-nlp\n",
        "\n",
        "# Choose between shuffled corpus, clean corpus or original corpus.\n",
        "\n",
        "\n",
        "#df=open('/content/output_backup.txt').read()\n",
        "#df=open('/content/output_shuffled.txt').read()\n",
        "df=open('/content/output_clean.txt').read()\n",
        "\n",
        "aug_w2v = naw.WordEmbsAug(model_type='glove', model_path='/content/glove.6B.100d.txt',\n",
        "    action=\"substitute\")\n",
        "aug_w2v.aug_p=0.2\n",
        "augmented_text = aug_w2v.augment(df)\n",
        "#This code append the result to our chosen corpus\n",
        "#print(augmented_text, file=open(\"output_clean.txt\", \"a\"))\n",
        "#print(augmented_text, file=open(\"output_backup.txt\", \"a\")) \n",
        "print(augmented_text, file=open(\"output_GloVe.txt\", \"a\"))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0IcmesTm5X2"
      },
      "source": [
        "#test on a small example (it can be run multiple times, the result will be appended on the .txt file)\n",
        "df=\"Expand Hobs Studio at London's HereEast campus)\"\n",
        "\n",
        "aug_w2v = naw.WordEmbsAug(model_type='glove', model_path='/content/glove.6B.100d.txt',\n",
        "    action=\"substitute\")\n",
        "aug_w2v.aug_p=0.2\n",
        "augmented_text = aug_w2v.augment(df)\n",
        "print(augmented_text, file=open(\"output_small_GloVe.txt\", \"a\"))"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKFz4CbeQPVb"
      },
      "source": [
        "#duplicating the tokenized corpus \n",
        "\n",
        "#get backup \n",
        "df = df_backup\n",
        "\n",
        "count = 0\n",
        "while count < 2:\n",
        "    \n",
        "    \n",
        "    print(df, file=open(\"output_duplicated.txt\", \"a\")) \n",
        "    count = count + 1 \n",
        "\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XtnjDyEzuC3w"
      },
      "source": [
        "# Load the chosen an option\n",
        "df= 'output_duplicated.txt'\n",
        "#df= 'output_backup.txt'\n",
        "#df= 'output_shuffled.txt'\n",
        "#df= 'output_clean.txt'\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g749O9iU12Gy"
      },
      "source": [
        "sess = gpt2.start_tf_sess()"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yhL9YPf813eL",
        "outputId": "4528507f-d2c2-4299-e013-21da1613cabc"
      },
      "source": [
        "# Adapted from https://minimaxir.com/2019/09/howto-gpt2/\n",
        "\n",
        "gpt2.finetune(sess,\n",
        "              dataset=df,\n",
        "              model_name='355M',# Choose your favourite model, I run all my tests with the 355M model\n",
        "              steps=50, #50 provides the best results\n",
        "              restore_from='fresh',\n",
        "              run_name='run1',\n",
        "              print_every=10,\n",
        "              sample_every=50,\n",
        "              save_every=10\n",
        "              )\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/gpt_2_simple/src/sample.py:17: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/gpt_2_simple/src/memory_saving_gradients.py:62: get_backward_walk_ops (from tensorflow.contrib.graph_editor.select) is deprecated and will be removed after 2019-06-06.\n",
            "Instructions for updating:\n",
            "Please use tensorflow.python.ops.op_selector.get_backward_walk_ops.\n",
            "Loading checkpoint models/355M/model.ckpt\n",
            "INFO:tensorflow:Restoring parameters from models/355M/model.ckpt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 364.41it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading dataset...\n",
            "dataset has 9343 tokens\n",
            "Training...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[10 | 23.81] loss=2.80 avg=2.80\n",
            "Saving checkpoint/run1/model-10\n",
            "[20 | 46.55] loss=1.24 avg=2.02\n",
            "Saving checkpoint/run1/model-20\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/training/saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n",
            "[30 | 84.16] loss=0.17 avg=1.39\n",
            "Saving checkpoint/run1/model-30\n",
            "[40 | 119.54] loss=0.13 avg=1.07\n",
            "Saving checkpoint/run1/model-40\n",
            "[50 | 154.99] loss=0.04 avg=0.86\n",
            "Saving checkpoint/run1/model-50\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 616
        },
        "id": "hTcHe87fCeY9",
        "outputId": "550224d0-63a2-475e-d149-cfd3601723ed"
      },
      "source": [
        "while True:\n",
        "  \n",
        "  ques = input(\"Please ask for something, I will be glad to share my knowledge : \")  \n",
        "\n",
        "# Choose between chatbot or 'dictionary'\n",
        "\n",
        "  inp = ques+'[Kadine] :' #chatbot\n",
        "  \n",
        "\n",
        "  chatbot = gpt2.generate(sess,\n",
        "                length=100,\n",
        "                top_p=0.0,\n",
        "                temperature = 0.6,  #higer is abstact, low is the same as input(between 0.6 and 1 works pretty well)\n",
        "                include_prefix=False,\n",
        "                prefix=inp,\n",
        "                nsamples=1,\n",
        "                batch_size=1,\n",
        "                return_as_list=True\n",
        "               \n",
        "                )\n",
        "  \n",
        "  # We need a strig to apply .split function, which will truncate the chatbot when a dot is found. \n",
        "  chatbot = str (chatbot) \n",
        "  chatbot = (chatbot).split(\".\")[0]\n",
        "  \n",
        "   \n",
        "  \n",
        "  print(chatbot)\n",
        "  \n",
        "  print(chatbot, file=open(\"transcript.txt\", \"a\"))\n",
        "  \n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Please ask for something, I will be glad to share my knowledge : Come i social media a tuo parere influenzano i nuovi artisti?\n",
            "['Come i social media a tuo parere influenzano i nuovi artisti?[Kadine] : no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    565\u001b[0m         \"\"\"\n\u001b[0;32m--> 566\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    567\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-db7926864ed2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0mques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Please ask for something, I will be glad to share my knowledge : \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Choose between chatbot or 'dictionary'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m         )\n\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}